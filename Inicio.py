# app.py ‚Äî Demo TF-IDF en Espa√±ol (mejorada)
import re
import unicodedata
from typing import List, Tuple, Dict

import numpy as np
import pandas as pd
import streamlit as st
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Opcional: stemming espa√±ol (con fallback "identidad" si falta NLTK)
try:
    from nltk.stem import SnowballStemmer
    STEMMER = SnowballStemmer("spanish")
    def stem_es(t: str) -> str: return STEMMER.stem(t)
except Exception:
    def stem_es(t: str) -> str: return t  # sin stemming si NLTK no est√°

st.set_page_config(page_title="Demo TF-IDF en Espa√±ol", page_icon="üîç", layout="wide")
st.title("üîç Demo TF-IDF en Espa√±ol ‚Äì Laboratorio")

# -----------------------------------------------------------------------------
# Datos de ejemplo
# -----------------------------------------------------------------------------
DEFAULT_DOCS = """El perro ladra fuerte en el parque.
El gato ma√∫lla suavemente durante la noche.
El perro y el gato juegan juntos en el jard√≠n.
Los ni√±os corren y se divierten en el parque.
La m√∫sica suena muy alta en la fiesta.
Los p√°jaros cantan hermosas melod√≠as al amanecer."""

SUGERIDAS = [
    "¬øD√≥nde juegan el perro y el gato?",
    "¬øQu√© hacen los ni√±os en el parque?",
    "¬øCu√°ndo cantan los p√°jaros?",
    "¬øD√≥nde suena la m√∫sica alta?",
    "¬øQu√© animal ma√∫lla durante la noche?",
]

# Stopwords ES compactas (sin depender de NLTK)
STOPWORDS_ES = set("""
a al algo algunas algunos ante antes as√≠ a√∫n cada como con contra cu√°ndo cu√°l cu√°les cuando de del desde donde dos el ella ellas ellos en entre era erais eran eras eres es esa esas ese eso esos esta estaban estado estaba est√°is estamos est√°n estar estar√° estas este √©stos esto estos estoy fue fuera fueron fui fuimos ha hab√≠an hab√≠a hab√≠ais hab√≠an hab√≠amos haber hace hacen hacer hacia han hasta hay la las le les lo los m√°s me mi mis mucha muchas mucho muchos muy nada ni no nos nosotras nosotros o os otra otras otro otros para pero poco por porque qu√© que quien qui√©n quienes se ser ser√° si sido sin sobre sois somos son soy su sus tambi√©n te tiene tienen tener tengo ten√≠a ten√≠an tenemos ten√©is tus tu un una uno unas unos ya
""".split())

# -----------------------------------------------------------------------------
# Utils de limpieza/tokenizaci√≥n
# -----------------------------------------------------------------------------
def strip_accents(s: str) -> str:
    # Normaliza acentos (cami√≥n -> camion)
    nkfd = unicodedata.normalize("NFKD", s)
    return "".join([c for c in nkfd if not unicodedata.combining(c)])

def tokenize_es(text: str, use_stemming: bool, remove_stopwords: bool) -> List[str]:
    # Min√∫sculas + quitar URLs/men√ß√µes
    text = text.lower()
    text = re.sub(r"https?://\S+|[@#]\w+", " ", text)
    # Mantener letras (incluye √±, √°√©√≠√≥√∫√º) y espacios; quitar n√∫meros/otros signos
    text = re.sub(r"[^a-z√°√©√≠√≥√∫√º√±\s]", " ", text)
    # Normalizar espacios
    text = re.sub(r"\s+", " ", text).strip()
    # Opcional: quitar acentos para emparejar mejor
    text = strip_accents(text)
    tokens = [t for t in text.split() if len(t) > 1]
    if remove_stopwords:
        tokens = [t for t in tokens if t not in STOPWORDS_ES]
    if use_stemming:
        tokens = [stem_es(t) for t in tokens]
    return tokens

def build_vectorizer(ngram_max: int, min_df: float, max_df: float,
                     use_stemming: bool, remove_stopwords: bool,
                     sublinear: bool, norm: str) -> TfidfVectorizer:
    return TfidfVectorizer(
        tokenizer=lambda s: tokenize_es(s, use_stemming=use_stemming, remove_stopwords=remove_stopwords),
        ngram_range=(1, ngram_max),
        min_df=min_df,            # puede ser int o proporci√≥n
        max_df=max_df,            # idem
        use_idf=True,
        sublinear_tf=sublinear,
        norm=norm,                # "l2" recomendado
    )

def highlight_terms(text: str, terms: List[str]) -> str:
    """Resalta t√©rminos (ya normalizados sin acentos) dentro de text (tal cual).
       Hacemos b√∫squeda case-insensitive y de palabra completa.
    """
    if not terms: return text
    # Construir patr√≥n con OR escapado
    safe = [re.escape(t) for t in sorted(set(terms), key=len, reverse=True)]
    if not safe: return text
    pattern = r"\b(" + "|".join(safe) + r")\b"
    def repl(m):
        return f"<mark>{m.group(0)}</mark>"
    # case-insensitive y sin acentos: aplicamos sobre versi√≥n sin acentos para localizar posiciones,
    # luego sustituimos sobre original con heur√≠stica sencilla
    # (estrategia simple: usamos regex sobre original pero con IGNORECASE)
    return re.sub(pattern, repl, text, flags=re.IGNORECASE)

# -----------------------------------------------------------------------------
# Sidebar ‚Äî Par√°metros
# -----------------------------------------------------------------------------
with st.sidebar:
    st.header("‚öôÔ∏è Par√°metros del vectorizador")
    ngram_max = st.select_slider("N-gramas (m√°x)", options=[1, 2, 3], value=2)
    min_df = st.number_input("min_df", value=1.0, min_value=0.0, step=0.1,
                             help="Si ‚â•1 se interpreta como conteo; si <1 como proporci√≥n del corpus.")
    max_df = st.number_input("max_df", value=1.0, min_value=0.0, step=0.1,
                             help="Filtra t√©rminos muy frecuentes (ruido). 1.0 = sin filtro")
    remove_stop = st.checkbox("Quitar stopwords", True)
    use_stem = st.checkbox("Aplicar stemming (Snowball)", True)
    sublinear = st.checkbox("Sublinear TF (log(1+tf))", True)
    norm = st.selectbox("Normalizaci√≥n", ["l2", None], index=0)

    st.markdown("---")
    topk_matrix = st.slider("Top-K t√©rminos para mostrar en matriz", 5, 50, 20, 1)
    show_all_tokens = st.checkbox("Mostrar TODAS las columnas (cuidado: puede ser grande)", False)

# -----------------------------------------------------------------------------
# Layout principal
# -----------------------------------------------------------------------------
col1, col2 = st.columns([2, 1])

with col1:
    text_input = st.text_area("üìù Documentos (uno por l√≠nea):", DEFAULT_DOCS, height=180)
    question = st.text_input("‚ùì Escribe tu pregunta:", "¬øD√≥nde juegan el perro y el gato?")

with col2:
    st.markdown("### üí° Preguntas sugeridas")
    for q in SUGERIDAS:
        if st.button(q, use_container_width=True):
            st.session_state.question = q
            st.rerun()

# Sincroniza si se escogi√≥ sugerida
if "question" in st.session_state:
    question = st.session_state.question

# -----------------------------------------------------------------------------
# Bot√≥n Analizar
# -----------------------------------------------------------------------------
if st.button("üîé Analizar", type="primary"):
    docs = [d.strip() for d in text_input.split("\n") if d.strip()]
    if len(docs) < 1:
        st.error("‚ö†Ô∏è Ingresa al menos un documento.")
        st.stop()
    if not question.strip():
        st.error("‚ö†Ô∏è Escribe una pregunta.")
        st.stop()

    # Construir vectorizador con par√°metros
    vec = build_vectorizer(
        ngram_max=ngram_max,
        min_df=min_df,
        max_df=max_df,
        use_stemming=use_stem,
        remove_stopwords=remove_stop,
        sublinear=sublinear,
        norm=norm,
    )

    # Ajuste con documentos
    X = vec.fit_transform(docs)            # (n_docs, n_terms)
    feature_names = np.array(vec.get_feature_names_out())

    # Vector de la pregunta
    q_vec = vec.transform([question])      # (1, n_terms)

    # Similitud coseno
    sims = cosine_similarity(q_vec, X).flatten()
    order = np.argsort(-sims)

    # ------------------- Vista de similitud por doc -------------------
    st.markdown("### üìà Similitud por documento")
    sim_table = pd.DataFrame({
        "Documento": [f"Doc {i+1}" for i in range(len(docs))],
        "Texto": docs,
        "Similitud": sims
    }).sort_values("Similitud", ascending=False)
    st.dataframe(sim_table.style.format({"Similitud": "{:.3f}"}), use_container_width=True, hide_index=True)

    # Mejor documento
    best_idx = int(order[0])
    best_doc = docs[best_idx]
    best_score = float(sims[best_idx])

    # ------------------- Matriz TF-IDF (recortada) -------------------
    st.markdown("### üìä Matriz TF-IDF")
    Xdense = X.toarray()
    if show_all_tokens:
        df_tfidf = pd.DataFrame(
            Xdense, columns=feature_names, index=[f"Doc {i+1}" for i in range(len(docs))]
        )
        st.dataframe(df_tfidf.round(3), use_container_width=True)
    else:
        # Elegir Top-K t√©rminos por varianza global para una vista compacta
        variances = Xdense.var(axis=0)
        topk_idx = np.argsort(-variances)[:topk_matrix]
        df_tfidf = pd.DataFrame(
            Xdense[:, topk_idx],
            columns=feature_names[topk_idx],
            index=[f"Doc {i+1}" for i in range(len(docs))]
        )
        st.dataframe(df_tfidf.round(3), use_container_width=True)

    # ------------------- T√©rminos relevantes del mejor doc -------------------
    st.markdown("### üè∑Ô∏è T√©rminos m√°s relevantes del documento ganador")
    row = X[best_idx, :].toarray().ravel()
    top_doc_idx = np.argsort(-row)[:10]
    top_doc_terms = feature_names[top_doc_idx]
    top_doc_scores = row[top_doc_idx]
    df_top_terms = pd.DataFrame({"T√©rmino": top_doc_terms, "TF-IDF": top_doc_scores})
    st.dataframe(df_top_terms.style.format({"TF-IDF": "{:.3f}"}), use_container_width=True, hide_index=True)

    # ------------------- Resaltado de la respuesta -------------------
    # Derivar t√©rminos de la pregunta (tokenizados) para resaltar
    q_terms = tokenize_es(question, use_stemming=use_stem, remove_stopwords=remove_stop)
    # Conserva solo t√©rminos que est√©n en el vocabulario (mejor se√±al)
    vocab_set = set(feature_names)
    to_highlight = [t for t in q_terms if t in vocab_set]
    highlighted = highlight_terms(best_doc, to_highlight)

    st.markdown("### üéØ Respuesta")
    st.markdown(f"**Tu pregunta:** {question}")
    if best_score > 0.05:
        st.success(f"**Documento m√°s similar (Doc {best_idx+1})** ‚Äî similitud: **{best_score:.3f}**")
    else:
        st.warning(f"**Documento m√°s similar (Doc {best_idx+1})** ‚Äî similitud baja: **{best_score:.3f}**")

    st.markdown(f"<div style='padding:.6rem 1rem;border-radius:12px;border:1px solid #e5e7eb;background:#f8fafc'>{highlighted}</div>", unsafe_allow_html=True)

    # ------------------- Explicaci√≥n breve -------------------
    with st.expander("‚ÑπÔ∏è ¬øC√≥mo funciona este demo?"):
        st.markdown("""
- **Limpieza** del texto ‚Üí min√∫sculas, sin URLs/acentos/signos.
- **Tokenizaci√≥n** (con *stemming* opcional) y eliminaci√≥n de **stopwords**.
- Construimos un espacio TF-IDF con **n-gramas**.
- Calculamos **similitud coseno** entre tu pregunta y cada documento.
- Mostramos el documento con mayor similitud y los **t√©rminos TF-IDF** m√°s fuertes.
        """)

