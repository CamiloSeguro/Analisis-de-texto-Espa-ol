# app.py ‚Äî Demo TF-IDF en Espa√±ol (actualizado con fixes min_df/max_df y validaciones)
import re
import unicodedata
from typing import List

import numpy as np
import pandas as pd
import streamlit as st
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Opcional: stemming espa√±ol (fallback si no est√° NLTK)
try:
    from nltk.stem import SnowballStemmer
    STEMMER = SnowballStemmer("spanish")
    def stem_es(t: str) -> str: return STEMMER.stem(t)
except Exception:
    def stem_es(t: str) -> str: return t

st.set_page_config(page_title="Demo TF-IDF en Espa√±ol", page_icon="üîç", layout="wide")
st.title("üîç Demo TF-IDF en Espa√±ol ‚Äì Laboratorio")

# ---------------------------------------------------------------------
# Datos de ejemplo
# ---------------------------------------------------------------------
DEFAULT_DOCS = """El perro ladra fuerte en el parque.
El gato ma√∫lla suavemente durante la noche.
El perro y el gato juegan juntos en el jard√≠n.
Los ni√±os corren y se divierten en el parque.
La m√∫sica suena muy alta en la fiesta.
Los p√°jaros cantan hermosas melod√≠as al amanecer."""

SUGERIDAS = [
    "¬øD√≥nde juegan el perro y el gato?",
    "¬øQu√© hacen los ni√±os en el parque?",
    "¬øCu√°ndo cantan los p√°jaros?",
    "¬øD√≥nde suena la m√∫sica alta?",
    "¬øQu√© animal ma√∫lla durante la noche?",
]

STOPWORDS_ES = set("""
a al algo algunas algunos ante antes as√≠ a√∫n cada como con contra cu√°ndo cu√°l cu√°les cuando de del desde donde dos el ella ellas ellos en entre era erais eran eras eres es esa esas ese eso esos esta estaban estado estaba est√°is estamos est√°n estar estar√° estas este √©stos esto estos estoy fue fuera fueron fui fuimos ha hab√≠an hab√≠a hab√≠ais hab√≠an hab√≠amos haber hace hacen hacer hacia han hasta hay la las le les lo los m√°s me mi mis mucha muchas mucho muchos muy nada ni no nos nosotras nosotros o os otra otras otro otros para pero poco por porque qu√© que quien qui√©n quienes se ser ser√° si sido sin sobre sois somos son soy su sus tambi√©n te tiene tienen tener tengo ten√≠a ten√≠an tenemos ten√©is tus tu un una uno unas unos ya
""".split())

# ---------------------------------------------------------------------
# Utils de limpieza/tokenizaci√≥n
# ---------------------------------------------------------------------
def strip_accents(s: str) -> str:
    nkfd = unicodedata.normalize("NFKD", s)
    return "".join([c for c in nkfd if not unicodedata.combining(c)])

def tokenize_es(text: str, use_stemming: bool, remove_stopwords: bool) -> List[str]:
    text = text.lower()
    text = re.sub(r"https?://\S+|[@#]\w+", " ", text)
    text = re.sub(r"[^a-z√°√©√≠√≥√∫√º√±\s]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    text = strip_accents(text)
    tokens = [t for t in text.split() if len(t) > 1]
    if remove_stopwords:
        tokens = [t for t in tokens if t not in STOPWORDS_ES]
    if use_stemming:
        tokens = [stem_es(t) for t in tokens]
    return tokens

def build_vectorizer(ngram_max: int, min_df, max_df,
                     use_stemming: bool, remove_stopwords: bool,
                     sublinear: bool, norm):
    return TfidfVectorizer(
        tokenizer=lambda s: tokenize_es(s, use_stemming=use_stemming, remove_stopwords=remove_stopwords),
        ngram_range=(1, ngram_max),
        min_df=min_df,             # puede ser int (conteo) o float (proporci√≥n)
        max_df=max_df,             # idem
        use_idf=True,
        sublinear_tf=sublinear,
        norm=norm,
    )

def highlight_terms(text: str, terms: List[str]) -> str:
    if not terms: return text
    safe = [re.escape(t) for t in sorted(set(terms), key=len, reverse=True)]
    if not safe: return text
    pattern = r"\b(" + "|".join(safe) + r")\b"
    return re.sub(pattern, lambda m: f"<mark>{m.group(0)}</mark>", text, flags=re.IGNORECASE)

# ---------------------------------------------------------------------
# Sidebar ‚Äî Par√°metros (incluye modo de df para evitar ValueError)
# ---------------------------------------------------------------------
with st.sidebar:
    st.header("‚öôÔ∏è Par√°metros del vectorizador")

    ngram_max = st.select_slider("N-gramas (m√°x)", options=[1, 2, 3], value=2)

    mode_df = st.radio("Modo de df", ["Conteo (int)", "Proporci√≥n (0‚Äì1)"], index=0, horizontal=True)
    if mode_df == "Conteo (int)":
        min_df_ui = st.number_input("min_df (conteo)", value=1, min_value=1, step=1,
                                    help="‚â•1 = al menos ese # de documentos. Recomendado: 1‚Äì2.")
        max_df_ui = st.number_input("max_df (conteo)", value=1000, min_value=1, step=1,
                                    help="Filtra t√©rminos demasiado frecuentes por conteo. D√©jalo alto para no filtrar.")
        min_df = int(min_df_ui)
        max_df = int(max_df_ui)
    else:
        min_df_ui = st.number_input("min_df (proporci√≥n 0‚Äì1)", value=0.0, min_value=0.0, max_value=1.0, step=0.05,
                                    help="0.0‚Äì1.0. 0.0 = sin filtro inferior. Evita 1.0 (100%).")
        max_df_ui = st.number_input("max_df (proporci√≥n 0‚Äì1)", value=1.0, min_value=0.0, max_value=1.0, step=0.05,
                                    help="Filtra t√©rminos muy frecuentes. 1.0 = sin filtro superior.")
        min_df = float(min_df_ui)
        max_df = float(max_df_ui)

    remove_stop = st.checkbox("Quitar stopwords", True)
    use_stem = st.checkbox("Aplicar stemming (Snowball)", True)
    sublinear = st.checkbox("Sublinear TF (log(1+tf))", True)
    norm = st.selectbox("Normalizaci√≥n", ["l2", None], index=0)

    st.markdown("---")
    topk_matrix = st.slider("Top-K t√©rminos para mostrar en matriz", 5, 50, 20, 1)
    show_all_tokens = st.checkbox("Mostrar TODAS las columnas (puede ser grande)", False)

# ---------------------------------------------------------------------
# Layout principal
# ---------------------------------------------------------------------
col1, col2 = st.columns([2, 1])
with col1:
    text_input = st.text_area("üìù Documentos (uno por l√≠nea):", DEFAULT_DOCS, height=180)
    question = st.text_input("‚ùì Escribe tu pregunta:", "¬øD√≥nde juegan el perro y el gato?")
with col2:
    st.markdown("### üí° Preguntas sugeridas")
    for q in SUGERIDAS:
        if st.button(q, use_container_width=True):
            st.session_state.question = q
            st.rerun()

if "question" in st.session_state:
    question = st.session_state.question

# ---------------------------------------------------------------------
# Bot√≥n Analizar
# ---------------------------------------------------------------------
if st.button("üîé Analizar", type="primary"):
    docs = [d.strip() for d in text_input.split("\n") if d.strip()]
    if len(docs) < 1:
        st.error("‚ö†Ô∏è Ingresa al menos un documento.")
        st.stop()
    if not question.strip():
        st.error("‚ö†Ô∏è Escribe una pregunta.")
        st.stop()

    # Diagn√≥stico: aseg√∫rate que no queden docs vac√≠os tras tokenizaci√≥n
    token_lists = [tokenize_es(d, use_stemming=use_stem, remove_stopwords=remove_stop) for d in docs]
    empty_docs = [i for i, toks in enumerate(token_lists) if len(toks) == 0]
    if empty_docs:
        st.error(
            "Estos documentos quedaron VAC√çOS tras la limpieza/tokenizaci√≥n: "
            + ", ".join([f"Doc {i+1}" for i in empty_docs])
            + ". Reduce filtros, desactiva stopwords/stemming o revisa el texto."
        )
        st.stop()

    vec = build_vectorizer(
        ngram_max=ngram_max,
        min_df=min_df,
        max_df=max_df,
        use_stemming=use_stem,
        remove_stopwords=remove_stop,
        sublinear=sublinear,
        norm=norm,
    )

    # Ajuste con manejo del ValueError t√≠pico de pruning
    try:
        X = vec.fit_transform(docs)   # (n_docs, n_terms)
    except ValueError:
        st.error("No quedaron t√©rminos despu√©s del pruning de TF-IDF.")
        st.info(
            "- Usa **min_df=1** (conteo) o baja la proporci√≥n (p.ej., 0.0‚Äì0.1).\n"
            "- Sube **max_df** (p.ej., 1.0 o un conteo grande).\n"
            "- Desactiva temporalmente **stopwords** y/o **stemming**.\n"
            "- Ampl√≠a **n-gramas** (2 o 3)."
        )
        st.stop()

    feature_names = np.array(vec.get_feature_names_out())
    q_vec = vec.transform([question])
    sims = cosine_similarity(q_vec, X).flatten()
    order = np.argsort(-sims)

    # --- Similitud por documento
    st.markdown("### üìà Similitud por documento")
    sim_table = pd.DataFrame({
        "Documento": [f"Doc {i+1}" for i in range(len(docs))],
        "Texto": docs,
        "Similitud": sims
    }).sort_values("Similitud", ascending=False)
    st.dataframe(sim_table.style.format({"Similitud": "{:.3f}"}), use_container_width=True, hide_index=True)

    best_idx = int(order[0])
    best_doc = docs[best_idx]
    best_score = float(sims[best_idx])

    # --- Matriz TF-IDF (recortada)
    st.markdown("### üìä Matriz TF-IDF")
    Xdense = X.toarray()
    if show_all_tokens:
        df_tfidf = pd.DataFrame(
            Xdense, columns=feature_names, index=[f"Doc {i+1}" for i in range(len(docs))]
        )
        st.dataframe(df_tfidf.round(3), use_container_width=True)
    else:
        variances = Xdense.var(axis=0)
        topk_idx = np.argsort(-variances)[:topk_matrix]
        df_tfidf = pd.DataFrame(
            Xdense[:, topk_idx],
            columns=feature_names[topk_idx],
            index=[f"Doc {i+1}" for i in range(len(docs))]
        )
        st.dataframe(df_tfidf.round(3), use_container_width=True)

    # --- T√©rminos top del documento ganador
    st.markdown("### üè∑Ô∏è T√©rminos m√°s relevantes del documento ganador")
    row = X[best_idx, :].toarray().ravel()
    top_doc_idx = np.argsort(-row)[:10]
    top_doc_terms = feature_names[top_doc_idx]
    top_doc_scores = row[top_doc_idx]
    df_top_terms = pd.DataFrame({"T√©rmino": top_doc_terms, "TF-IDF": top_doc_scores})
    st.dataframe(df_top_terms.style.format({"TF-IDF": "{:.3f}"}), use_container_width=True, hide_index=True)

    # --- Resaltado de la respuesta
    q_terms = tokenize_es(question, use_stemming=use_stem, remove_stopwords=remove_stop)
    vocab_set = set(feature_names)
    to_highlight = [t for t in q_terms if t in vocab_set]
    highlighted = highlight_terms(best_doc, to_highlight)

    st.markdown("### üéØ Respuesta")
    st.markdown(f"**Tu pregunta:** {question}")
    if best_score > 0.05:
        st.success(f"**Documento m√°s similar (Doc {best_idx+1})** ‚Äî similitud: **{best_score:.3f}**")
    else:
        st.warning(f"**Documento m√°s similar (Doc {best_idx+1})** ‚Äî similitud baja: **{best_score:.3f}**")
    st.markdown(
        f"<div style='padding:.6rem 1rem;border-radius:12px;border:1px solid #e5e7eb;background:#f8fafc'>{highlighted}</div>",
        unsafe_allow_html=True
    )

    with st.expander("‚ÑπÔ∏è ¬øC√≥mo funciona este demo?"):
        st.markdown("""
- **Limpieza** del texto ‚Üí min√∫sculas, sin URLs/acentos/signos.
- **Tokenizaci√≥n** (con *stemming* opcional) y eliminaci√≥n de **stopwords**.
- Espacio TF-IDF con **n-gramas** configurables.
- **Similitud coseno** entre tu pregunta y cada documento.
- Documento con mayor similitud + **t√©rminos TF-IDF** m√°s fuertes.
        """)
